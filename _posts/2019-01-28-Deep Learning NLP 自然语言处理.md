# NLP 自然语言处理



### NLP 是什么？解决什么问题？怎么做？

##### 1. 语言是什么

如果从信息学的角度去看语言，语言也是一种编码。

- 发起人的思想
- 通过 某种编码方式，成为一串符号（可以表现为中文，英文，或者其他01的二进制）
- 接受人收到这一串符号，进行解码，
- 通过解码之后，接受人就收到了发起人想要传递给他的 思想



##### 2. 了解了上面语言的本质，那么从本质上来说，人类的几百种语言和01的电脑语言根本上没有不同

根据上面的定义来说，我们可以重新定义我们的很多工作

- 翻译，比如中英两种语言，就是将A的中文思想传递给B，B理解之后再通过英文传递给C。然后再反过来。

  这里就可以看出翻译这种古老工种的落后了，

- 机器翻译，就是A的思想通过某种语言（如中文）给机器B，B经过解码之后，换成一种新的形式（如英文）还给A

- 语音控制，那么自然就是A的思想通过某种语言（如中文）给机器B，B就直接将他们转化为他所认识的01的机器语言，来控制相应的电子设备。

  

##### 3. 除此之外还可以有很多，包括绘画，音乐，文学、电影、游戏、戏剧，他们从表面上来看是一段故事，其实本质上都是思想在不同实体上的映射，从而将思想在人和人之间进行着传递。



##### 4. NLP走过的弯路 

从20世纪50年代-70年代，全世界科学家对计算机处理自然语言的认知都局限在人类学习语言的方式上，也就是说用电脑模拟人脑，像比如专家系统，知识库系统（Knowledge Base），但20多年的结果是毫无所得。

- 这个倒的确是和alphaGo Lee的版本很像，输入了太多人类的 **成功** 棋谱之后，严重压缩了它的发展空间

##### 5. 基于数学模型和统计学的阶段

后来NLP就进入了基于数学模型和统计学的阶段，就是不再考虑让电脑去记住什么单词，而是完全通过统计学的模型，得出对各种单词的理解和认识，在这个阶段，人类取得了很大的进步。

根据这个阶段的认知，电脑本身不需要知道某个单词的含义，而是根据互相之间的关联次数，得出这个单词可能表示某个意思的概率是多少。（是不是很像图像识别）

贴一个超级简化版本的单词统计表，单词和单词之间的互相的关联程度和概率，代表了该单词的含义



![image-20190129190037570](https://ws2.sinaimg.cn/large/006tNc79gy1fznnplkmwzj30ou0nm43v.jpg)



### 上图所用到的就是：Word2Vec，具体地说就是skip-Gram算法

基本逻辑图如下：

![image-20190129191233160](https://ws1.sinaimg.cn/large/006tNc79gy1fzno22s3qzj314b0n0n5r.jpg)



##### 如上所示非常简单

- Step1: 获得一个超大的单词库

- Step2: 给每个词进行嵌入，如$V_{cat}$

- Step3: 预测$V_{cat}$的和其他词的连接的概率

  - $$
    wV_{cat} + b
    $$

    

- Step 4: 将结果进行softmax 操作，这里用了一个技巧，叫sampled softmax。于是就可以得到 $V_{cat}$ 和那些的关系紧密
- 然后重复1-4对应所有的词



### 更详细的介绍，这里贴上一个知乎上的回答，有兴趣的可以了解更多

Word2Vec介绍：直观理解skip-gram模型 <https://zhuanlan.zhihu.com/p/29305464>



### 相关资料

- udacity <https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/65932489170923>

- 一文详解 Word2vec 之 Skip-Gram 模型（结构篇）<https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html>

- word2vec tutorial - the skip-gram model <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/>

- 数学之美 （作者：吴军）

- Word2Vec介绍：直观理解skip-gram模型 <https://zhuanlan.zhihu.com/p/29305464>